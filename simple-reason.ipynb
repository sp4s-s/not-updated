{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d7f6da0e-5985-4411-ba14-3c6114d0063a","cell_type":"code","source":"! pip list","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"id":"ab97b648-85cc-40a6-b0d4-25107ae7cb68","cell_type":"code","source":"%env HF_ENABLE_HF_TRANSFER=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:03:38.277469Z","iopub.execute_input":"2025-07-16T15:03:38.277726Z","iopub.status.idle":"2025-07-16T15:03:38.285066Z","shell.execute_reply.started":"2025-07-16T15:03:38.277698Z","shell.execute_reply":"2025-07-16T15:03:38.284339Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"env: HF_ENABLE_HF_TRANSFER=1\n","output_type":"stream"}],"execution_count":1},{"id":"2522d540","cell_type":"code","source":"!pip install unsloth vllm -qU\n!pip install --upgrade pillow -qU\n# If you are running this notebook locally (not colab), you need to install `diffusers` too\n!pip install diffusers wandb -qU\n\n# Temporarily install a specific TRL nightly version that supports GRPO or just git dev version\n!pip install git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b -qU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:03:41.725617Z","iopub.execute_input":"2025-07-16T15:03:41.725887Z","iopub.status.idle":"2025-07-16T15:07:42.549317Z","shell.execute_reply.started":"2025-07-16T15:03:41.725867Z","shell.execute_reply":"2025-07-16T15:07:42.548356Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.5/297.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.4/383.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.6/734.6 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.6/856.6 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.8/162.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.5/385.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.5/950.5 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ndistributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\nydata-profiling 4.16.1 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for trl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":2},{"id":"5f9c8219-d084-4782-a94a-a73f069ad355","cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF\")\nlogin(hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:08:12.589847Z","iopub.execute_input":"2025-07-16T15:08:12.590241Z","iopub.status.idle":"2025-07-16T15:08:13.378163Z","shell.execute_reply.started":"2025-07-16T15:08:12.590213Z","shell.execute_reply":"2025-07-16T15:08:13.377433Z"}},"outputs":[],"execution_count":3},{"id":"dfc3383a-18e0-4565-ba4a-3764ae54fae1","cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom wandb import login\nuser_secrets = UserSecretsClient()\n\nwb_token = user_secrets.get_secret(\"WB\")\nlogin(key=wb_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:08:16.680572Z","iopub.execute_input":"2025-07-16T15:08:16.680844Z","iopub.status.idle":"2025-07-16T15:08:24.430978Z","shell.execute_reply.started":"2025-07-16T15:08:16.680822Z","shell.execute_reply":"2025-07-16T15:08:24.430382Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msp4ss\u001b[0m (\u001b[33msp4ss-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"id":"0c45766e","cell_type":"code","source":"import wandb\n\nwandb.init(\n    project=\"Qwen2.5-3B-Instruct-reason\",\n    entity=\"sp4ss-self\",\n    config={\n        'learning_rate': 5e-6,\n        'batch_size': 1,\n        'gradient_accumulation': 1,\n        'adam_beta1': 0.9,\n        'adam_beta2': 0.99,\n        'weight_decay': 0.1,\n        'warmup_ratio': 0.1,\n        'lr_scheduler_type': \"cosine\",\n        'optim': \"adamw_8bit\",\n    },\n    name=\"COT-qwen2.5-3b\",\n    notes=\"Experiment to tune model Qwen\",\n    tags=[\"grpo\", \"gsm8k\", \"LoRA\"],\n    job_type=\"training\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:15:47.667663Z","iopub.execute_input":"2025-07-16T15:15:47.667911Z","iopub.status.idle":"2025-07-16T15:15:54.349734Z","shell.execute_reply.started":"2025-07-16T15:15:47.667895Z","shell.execute_reply":"2025-07-16T15:15:54.349148Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing previous runs because reinit is set to 'default'."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">COT-qwen2.5-3b</strong> at: <a href='https://wandb.ai/sp4ss-self/Qwen2.5-3B-Instruct-reason/runs/9xnuriuy' target=\"_blank\">https://wandb.ai/sp4ss-self/Qwen2.5-3B-Instruct-reason/runs/9xnuriuy</a><br> View project at: <a href='https://wandb.ai/sp4ss-self/Qwen2.5-3B-Instruct-reason' target=\"_blank\">https://wandb.ai/sp4ss-self/Qwen2.5-3B-Instruct-reason</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250716_151541-9xnuriuy/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250716_151547-f9468rc0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sp4ss-self/Qwen2.5-3B-Instruct-reason/runs/f9468rc0' target=\"_blank\">COT-qwen2.5-3b</a></strong> to <a href='https://wandb.ai/sp4ss-self/Qwen2.5-3B-Instruct-reason' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sp4ss-self/Qwen2.5-3B-Instruct-reason' target=\"_blank\">https://wandb.ai/sp4ss-self/Qwen2.5-3B-Instruct-reason</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sp4ss-self/Qwen2.5-3B-Instruct-reason/runs/f9468rc0' target=\"_blank\">https://wandb.ai/sp4ss-self/Qwen2.5-3B-Instruct-reason/runs/f9468rc0</a>"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sp4ss-self/Qwen2.5-3B-Instruct-reason/runs/f9468rc0?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x79eb61776850>"},"metadata":{}}],"execution_count":14},{"id":"c3d6ab5d","cell_type":"code","source":"from unsloth import FastLanguageModel, PatchFastRL\n# Execute the Patch\nPatchFastRL(\"GRPO\", FastLanguageModel)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:09:21.559820Z","iopub.execute_input":"2025-07-16T15:09:21.560125Z","iopub.status.idle":"2025-07-16T15:09:54.792744Z","shell.execute_reply.started":"2025-07-16T15:09:21.560103Z","shell.execute_reply":"2025-07-16T15:09:54.791986Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-07-16 15:09:31.786033: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752678571.993189      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752678572.048434      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\nINFO 07-16 15:09:49 [__init__.py:244] Automatically detected platform cuda.\n","output_type":"stream"}],"execution_count":6},{"id":"02cae910","cell_type":"code","source":"import torch\n\n# Load Base Model & Tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n    max_seq_length = 2048,                      # Can increase for longer reasoning traces\n    load_in_4bit = True,                        # False for LoRA 16bit\n    fast_inference = True,                      # Enable vLLM fast inference\n    max_lora_rank = 64,                         # Larger rank = smarter, but slower\n    gpu_memory_utilization = 0.7,               # Reduce if out of memory\n)\n\n# Prepare Model for Parameter Efficient Fine Tuning\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 64,                                     # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    lora_alpha = 64,                            # LoRA Rank\n    use_gradient_checkpointing = \"unsloth\",     # Enable long context finetuning\n    random_state = 3407,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:10:29.365251Z","iopub.execute_input":"2025-07-16T15:10:29.365486Z","iopub.status.idle":"2025-07-16T15:10:44.191929Z","shell.execute_reply.started":"2025-07-16T15:10:29.365461Z","shell.execute_reply":"2025-07-16T15:10:44.191113Z"}},"outputs":[{"name":"stdout","text":"Unsloth: vLLM does not work on older GPUs - will switch to Unsloth inference!\n==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.52.4. vLLM: 0.9.2.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"}],"execution_count":8},{"id":"e12bb6aa","cell_type":"code","source":"SYSTEM_PROMPT = \"\"\"\nPlease follow this structure when responding:\n\n<reasoning>\nProvide a detailed explanation of your thought process, the steps you took to arrive at the conclusion, and any relevant assumptions or considerations. This section should break down the logic behind your answer.\n</reasoning>\n\n<answer>\nPresent your final response in a concise and clear manner, based on the reasoning above. The answer should directly address the user's question or request.\n</answer>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:10:50.242528Z","iopub.execute_input":"2025-07-16T15:10:50.243255Z","iopub.status.idle":"2025-07-16T15:10:50.246621Z","shell.execute_reply.started":"2025-07-16T15:10:50.243229Z","shell.execute_reply":"2025-07-16T15:10:50.246017Z"}},"outputs":[],"execution_count":9},{"id":"9af21cae","cell_type":"code","source":"# SYSTEM_PROMPT = \"\"\"\n# Respond in the following format:\n# <reasoning>\n# ...\n# </reasoning>\n# <answer>\n# ...\n# </answer>\n# \"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"04c7ee44","cell_type":"code","source":"import re\nfrom datasets import load_dataset, Dataset\n\n# Helper Function for Parsing Answer\ndef extract_hash_answer(text: str) -> str | None:\n    if \"####\" not in text:\n        return None\n    return text.split(\"####\")[1].strip()\n\n# uncomment middle messages for 1-shot prompting\ndef get_gsm8k_questions(split = \"train\") -> Dataset:\n    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n    data = data.map(lambda x: { # type: ignore\n        'prompt': [\n            {'role': 'system', 'content': SYSTEM_PROMPT},\n            {'role': 'user', 'content': x['question']}\n        ],\n        'answer': extract_hash_answer(x['answer'])\n    }) # type: ignore\n    return data # type: ignore\n\ndataset = get_gsm8k_questions()\n\n\n# Helper Function to Extract Answer from LLM Response\ndef extract_xml_answer(text: str) -> str:\n    answer = text.split(\"<answer>\")[-1]\n    answer = answer.split(\"</answer>\")[0]\n    return answer.strip()\n\ndef correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n    responses = [completion[0]['content'] for completion in completions]\n    q = prompts[0][-1]['content']\n    extracted_responses = [extract_xml_answer(r) for r in responses]\n    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n\n\ndef int_reward_func(completions, **kwargs) -> list[float]:\n    responses = [completion[0]['content'] for completion in completions]\n    extracted_responses = [extract_xml_answer(r) for r in responses]\n    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n\ndef strict_format_reward_func(completions, **kwargs) -> list[float]:\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n    responses = [completion[0][\"content\"] for completion in completions]\n    matches = [re.match(pattern, r) for r in responses]\n    return [0.5 if match else 0.0 for match in matches]\n\ndef soft_format_reward_func(completions, **kwargs) -> list[float]:\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n    responses = [completion[0][\"content\"] for completion in completions]\n    matches = [re.match(pattern, r) for r in responses]\n    return [0.5 if match else 0.0 for match in matches]\n\n# XML Counting Helper Function\ndef count_xml(text) -> float:\n    count = 0.0\n    if text.count(\"<reasoning>\\n\") == 1:\n        count += 0.125\n    if text.count(\"\\n</reasoning>\\n\") == 1:\n        count += 0.125\n    if text.count(\"\\n<answer>\\n\") == 1:\n        count += 0.125\n        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n    if text.count(\"\\n</answer>\") == 1:\n        count += 0.125\n        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n    return count\n\ndef xmlcount_reward_func(completions, **kwargs) -> list[float]:\n    contents = [completion[0][\"content\"] for completion in completions]\n    return [count_xml(c) for c in contents]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:11:02.752719Z","iopub.execute_input":"2025-07-16T15:11:02.753347Z","iopub.status.idle":"2025-07-16T15:11:05.510494Z","shell.execute_reply.started":"2025-07-16T15:11:02.753322Z","shell.execute_reply":"2025-07-16T15:11:05.509561Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f30816f6d724da1999e9e5560c9d58a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75762dc2eb71459f993fc4d4a7984a77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e906eee47704616a5117dad7e81055d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bbbbb7615af405e9d21cf21613b12f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd7a97bfeba640b68cfb8174017f0de8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a191c69ef9964958b1e2645d535fbafb"}},"metadata":{}}],"execution_count":10},{"id":"cf030cec","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dfa6aeb0","cell_type":"code","source":"from trl import GRPOConfig, GRPOTrainer\nfrom unsloth import is_bfloat16_supported\n\n# Defining Trainer Arguments\ntraining_args = GRPOConfig(\n    use_vllm = False,                             # Enable faster inference using vLLM\n    learning_rate = 5e-6,                        # Small learning rate for stable training\n    adam_beta1 = 0.9,                           # AdamW optimizer momentum parameter\n    adam_beta2 = 0.99,                          # AdamW optimizer second moment parameter\n    weight_decay = 0.1,                         # L2 regularization to prevent overfitting\n    warmup_ratio = 0.1,                         # Portion of training steps for learning rate warmup\n    lr_scheduler_type = \"cosine\",               # Learning rate decay schedule type\n    optim = \"adamw_8bit\",                       # Use 8-bit AdamW optimizer for memory efficiency\n    logging_steps = 1,                          # Log metrics every step\n    bf16 = is_bfloat16_supported(),            # Use bfloat16 if hardware supports it\n    fp16 = not is_bfloat16_supported(),        # Fallback to float16 if bfloat16 not supported\n    per_device_train_batch_size = 1,           # Number of prompts per GPU\n    gradient_accumulation_steps = 1,           # Number of steps to accumulate gradients\n    num_generations = 8,                       # Number of responses to generate per prompt for GRPO\n    max_prompt_length = 256,                   # Maximum length of input prompts in tokens\n    max_completion_length = 512,               # Maximum length of model responses in tokens\n    max_steps = 2000,                         # Total number of training steps\n    save_steps = 250,                         # Save checkpoint every 250 steps\n    max_grad_norm = 0.1,                      # Gradient clipping threshold\n    report_to = \"wandb\",                      # Log metrics to Weights & Biases\n    output_dir = \"outputs\",                   # Directory to save model checkpoints\n)\n\ntrainer = GRPOTrainer(\n    model = model,\n    processing_class = tokenizer,\n    reward_funcs = [\n        xmlcount_reward_func,\n        soft_format_reward_func,\n        strict_format_reward_func,\n        int_reward_func,\n        correctness_reward_func,\n    ],\n    args = training_args,\n    train_dataset = dataset,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:18:07.445449Z","iopub.execute_input":"2025-07-16T15:18:07.445740Z","iopub.status.idle":"2025-07-16T15:18:07.503548Z","shell.execute_reply.started":"2025-07-16T15:18:07.445718Z","shell.execute_reply":"2025-07-16T15:18:07.502867Z"}},"outputs":[{"name":"stdout","text":"Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\nWe will change the batch size of 1 to the `num_generations` of 8\n","output_type":"stream"}],"execution_count":16},{"id":"53eec7aa","cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:18:21.604635Z","iopub.execute_input":"2025-07-16T15:18:21.605236Z","iopub.status.idle":"2025-07-16T15:23:46.586290Z","shell.execute_reply.started":"2025-07-16T15:18:21.605214Z","shell.execute_reply":"2025-07-16T15:23:46.585116Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 7,473 | Num Epochs = 3 | Total steps = 2,000\nO^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n \"-____-\"     Trainable parameters = 119,734,272 of 3,205,672,960 (3.74% trained)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"-------------------- Question:\nA concert ticket costs $40. Mr. Benson bought 12 tickets and received a 5% discount for every ticket bought that exceeds 10. How much did Mr. Benson pay in all? \nAnswer:\n476 \nResponse:\nTo solve this problem, let's break it down into steps:\n\n1. **Calculate the cost for the first 10 tickets:**\n   - Since the discount applies to any ticket bought that exceeds 10, the first 10 tickets are not discounted.\n   - The cost for each of the first 10 tickets is $40.\n   - Therefore, the total cost for the first 10 tickets is 10 * $40 = $400.\n\n2. **Calculate the cost for the remaining 2 tickets:**\n   - For the tickets bought beyond the initial 10, a 5% discount is applied.\n   - The cost for each of the remaining 2 tickets after the discount is $40 * (1 - 0.05) = $40 * 0.95 = $38.\n   - The total cost for the remaining 2 tickets is 2 * $38 = $76.\n\n3. **Calculate the total amount paid:**\n   - Add the cost of the first 10 tickets and the cost of the remaining 2 tickets.\n   - Total amount paid = $400 + $76 = $476.\n\nTherefore, the total amount Mr. Benson paid for all the tickets is $476.\n\n<reasoning>\nIn this problem, we first calculated the cost of the first 10 tickets, which are bought at the full price of $40 each, resulting in $40 per ticket times 10, or $400. For the remaining 2 tickets, which are bought after the initial 10 at a 5% discount, we adjust the price to $40 * 0.95. We then sum the cost of these two groups to obtain the total amount paid, which is $400 plus $76. This logical approach ensures we account for all tickets and their respective discounts, leading to an accurate total cost calculation.\n</reasoning>\n\n<answer>\nMr. Benson paid a total of $476 for the 12 concert tickets, considering the 5% discount on the tickets bought beyond the first 10.\n</answer> \nExtracted:\nMr. Benson paid a total of $476 for the 12 concert tickets, considering the 5% discount on the tickets bought beyond the first 10.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mGPUTooOldForTriton\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n","\u001b[0;32m/kaggle/working/unsloth_compiled_cache/UnslothGRPOTrainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m                 \u001b[0;31m# to ensure backwards compatibility with trl 0.15.2 and maybe even 0.17\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m                 loss, completion_length, mean_kl = grpo_accumulated_loss(\n\u001b[0m\u001b[1;32m   1370\u001b[0m                     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m                     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/unsloth_compiled_cache/UnslothGRPOTrainer.py\u001b[0m in \u001b[0;36mgrpo_accumulated_loss\u001b[0;34m(trainer, input_ids, attention_mask, logits_to_keep, completion_mask, advantages, old_hidden_states, n_chunks, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         ).logits\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         loss, completion_length, mean_kl = UnslothEfficientGRPO.apply(\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0mnew_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mold_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/unsloth_compiled_cache/UnslothGRPOTrainer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, _new_hidden_states, _old_hidden_states, _ref_hidden_states, lm_head, _input_ids, _mask, _advantages, beta, scaler, n_chunks, extra_kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mgrad_inputs_j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccumulate_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_hidden_states_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_hidden_states_j\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref_hidden_states_j\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0minput_ids_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantages_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m                     \u001b[0;31m# Failures in the backend likely don't have useful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m                     \u001b[0;31m# data in the TorchDynamo frames, so we strip them out.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_dynamo_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# see TORCHDYNAMO_VERBOSE=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                     \u001b[0;31m# Restore the dynamic layer stack depth if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36mcreate_backend\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   3953\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdevice_props\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3954\u001b[0m             ):\n\u001b[0;32m-> 3955\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mGPUTooOldForTriton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_props\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3956\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mis_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mps\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3957\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTritonMissing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mGPUTooOldForTriton\u001b[0m: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"],"ename":"GPUTooOldForTriton","evalue":"Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n","output_type":"error"}],"execution_count":17},{"id":"c7a39b0c","cell_type":"code","source":"# Save the LoRA Adapter\nmodel.save_lora(\"grpo_saved_lora\")\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nuser_secrets = UserSecretsClient()\nhf = user_secrets.get_secret(\"HF\")\nlogin(hf)\n# Merge to 16bit - Replace with your HF Repo, Model Name, and HF Token\nif True: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\nif True: model.push_to_hub_merged(\"Pingsz/Qwen2.5-3B-Instruct-reason\", tokenizer, save_method = \"merged_16bit\", token = hf)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4b44b6c0","cell_type":"code","source":"# Test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5d1684be","cell_type":"code","source":"%%capture\n!pip install unsloth vllm\n!pip install --upgrade pillow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"828e445a","cell_type":"code","source":"from unsloth import FastLanguageModel\nfrom vllm import SamplingParams\nimport torch\n\n# Load the Model & Tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"Pingsz/Qwen2.5-3B-Instruct-reason\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n    fast_inference = True,\n    gpu_memory_utilization = 0.7,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0bae70b8","cell_type":"code","source":"PROMPT = \"How many r's are in the word strawberry?\"\n\n# SYSTEM_PROMPT = \"\"\"\n# Respond in the following format:\n# <reasoning>\n# ...\n# </reasoning>\n# <answer>\n# ...\n# </answer>\n# \"\"\"\n\ntext = tokenizer.apply_chat_template([\n    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n    {\"role\" : \"user\", \"content\" : PROMPT},\n], tokenize = False, add_generation_prompt = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bee8e24f","cell_type":"code","source":"#Generate sequence\nsampling_params = SamplingParams(\n    temperature = 0.8,\n    top_p = 0.95,\n    max_tokens = 1024,\n)\noutput = model.fast_generate(\n    text,\n    sampling_params = sampling_params,\n)[0].outputs[0].text\n\nprint(\"\\n\\n\", output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"af39c39c","cell_type":"code","source":"! pip freeze > requirements.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"baaed480","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d9777f84","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d7c0fa9e","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}